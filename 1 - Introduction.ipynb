{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "__Notes on workshop__: The goal of these workshops is not to teach the finer points of deep learning modeling but to focus on the mechanics of tensorflow. They rely heavily on two sets of tutorials found here:\n",
    "- https://github.com/easy-tensorflow/easy-tensorflow\n",
    "- https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "\n",
    "These are both great ressources and have a number of tutorials that get more in detail into a number of areas. Please refer to them for further learning.\n",
    "________________________\n",
    "\n",
    "\n",
    "The following is a simple implemetation of a deep learning model using the keras wrapper. These types of implementations work very well for most applications. The keras wrapper uses tensorflow as a backend and wraps it with some powerful API allowing for simple \n",
    "\n",
    "- Design of a model architecture\n",
    "- Creation of optimization rules\n",
    "- Input pipelines with existing datasets\n",
    "- Model training/evaluation/inference handling\n",
    "    \n",
    "    \n",
    "Additionally tensorflow has gone all in with the keras wrapper and now works interchangably with keras. This means packaged keras deep learning architecture layers can be used within tensorflow models and vice versa. This notebook focuses on a simple Fully Connected model architecture using the keras wrappers for implenentation. \n",
    "\n",
    "\n",
    "\n",
    "This section the higher level keras API's to demonstrate all the components needed for a deep learning model but will the majority of the workshop focus on learning how to use the low level tensorflow tools to achieve the same effect. A knowledge of tensorflows basics will:\n",
    "\n",
    "    1) Allow you more flexibility in developing model architectures.\n",
    "    2) Allow you to use and modify state of the art architectures which are generally available in tensorflow or pytorch.\n",
    "\n",
    "Throughout this workshop priorety will be given to learning tensorflow and details about modeling in general will be assumed knowledge. We will use the 'mnist' dataset and classification task for this series of notebooks. (http://yann.lecun.com/exdb/mnist/ ) \n",
    "\n",
    "\n",
    "## An entire model\n",
    "\n",
    "With the keras wrapper defining an entire deep learning pipeline can be done in a few lines of code. Below is an example of a fully connected model applied to the MNIST classification problem.\n",
    "\n",
    "This involves:\n",
    "\n",
    "    1) Loading the data\n",
    "    2) Defining a model architecture\n",
    "    3) Defining a loss or cost function\n",
    "    4) Defining an optimization rule\n",
    "    5) Train the model\n",
    "    6) Evaluate the model\n",
    "    \n",
    "    \n",
    "Try running a complete pipeline with the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# load the mnist dataset\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# define the models architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Define the backpropocgation optimization rule\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model for 5 epochs\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# evaluate the model by the defined metric\n",
    "eval = model.evaluate(x_test, y_test)\n",
    "print(\"Models final Loss {0:.4f} and accuracy {1:.2f}%\".format(eval[0],eval[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done...\n",
    "\n",
    "Lets now break this down into the component parts of the modeling pipeline. First the MNIST data is loaded and broken into a training and test set.\n",
    "\n",
    "\n",
    "## Loading the data set\n",
    "\n",
    "In this case the data is loaded into numpy arrays that is one effective method of preloading data for a tensorflow pipeline. A more effective manner can be to use the tf.data package. More information can be found at https://www.tensorflow.org/guide/datasets and a nice tutorial can be found here: https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "\n",
    "Second the deep learning model is defined. One of the great advantages of neural networks is the ability to piece together different \"layer\" structures to create different learning architectures that can be specialized to the learning task at hand. The vast majority of these architectures end up being component functions (input -> output) that can be stacked ontop of eachother. The keras Sequential model allows for stacks of layers to be customly defined. \n",
    "\n",
    "_Given your knowledge of deep learning architectures try to describe the model defined below._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: Tensorflow has been moving to use keras as the primary wrapper for it's precoded layer API in the past few versions and most of the available ready made  deep learning layers you will need are detailed here https://www.tensorflow.org/api_docs/python/tf/keras/layers\n",
    "\n",
    "\n",
    "# Creating the optimization rule\n",
    "\n",
    "Next we need to define the loss of a deep learning model and define the parameter optimization rule. This is done in a single step with the keras API. Where we simply select an optimization rule (Adam) and a loss metric (sparse_categorical_crossentropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "Finally we train the model. This will look much like the sci-kit pipeline. Where epoch is the number of times to run through the data. All the details of training are taken care for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The model can then be evaluated by any metrics defined in the model in this case we have the loss function and the accuracy as defined in the previous step. These metrics are calculated over a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = model.evaluate(x_test, y_test)\n",
    "print(\"Models final Loss {0:.4f} and accuracy {1:.2f}%\".format(eval[0],eval[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Finally the model can be used for inference by using the _model.predict_ function. The below is a snippet that allows for inference on an individual model, although the model takes data in batch form. Try a few different samples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thes sample index to be predicted on\n",
    "test_idx = 1\n",
    "\n",
    "### The sample must be put in batch form shape.\n",
    "test_img = x_test[test_idx].reshape([1,28,28])\n",
    "\n",
    "### the predict function is used to evaluate a single sample. The output is the individual \n",
    "### logits for each possible outcome.\n",
    "pred = np.argmax(model.predict(x_test[test_idx].reshape([1,28,28])))\n",
    "\n",
    "label = y_test[test_idx]\n",
    "print('The models prediction is %d and the label is %d.' % (pred,label))\n",
    "plt.imshow(test_img[0], cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "While this shows the different components of a deep learning modeling pipeline the keras wrappers hide most of the detail neccesary for more advanced modeling. That being said it can be useful for most applications and allows for very fast development. The rest of these workshops will be focused on learning the nitty gritty of tensorflow and how it can utilized to design very customizable models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
